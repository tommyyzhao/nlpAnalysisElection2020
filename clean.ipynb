{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clean","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vO8DTFq8KZ2R"},"source":["import json\n","import os\n","from tqdm import tqdm\n","import csv\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kWzrL7_CWXK","executionInfo":{"status":"ok","timestamp":1607039971224,"user_tz":300,"elapsed":395,"user":{"displayName":"Collin Kovacs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDgAXzo7M5lUBaQyUGIfe8abPgCc4Ro0dDc6j6Ug=s64","userId":"13043862019868328475"}},"outputId":"61a242ad-135f-4068-d7c1-1058c287f4b5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"nMUODX-2-0-S","executionInfo":{"status":"ok","timestamp":1607039971867,"user_tz":300,"elapsed":396,"user":{"displayName":"Collin Kovacs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDgAXzo7M5lUBaQyUGIfe8abPgCc4Ro0dDc6j6Ug=s64","userId":"13043862019868328475"}},"outputId":"1f799ab9-7ffc-4ad2-9ab9-02db8002402c"},"source":["os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"XZ2gXumcK1zq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607039973078,"user_tz":300,"elapsed":334,"user":{"displayName":"Collin Kovacs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDgAXzo7M5lUBaQyUGIfe8abPgCc4Ro0dDc6j6Ug=s64","userId":"13043862019868328475"}},"outputId":"53f01bcd-31e5-453a-f7f3-00b7b039a2c5"},"source":["interim_path = os.path.join('drive','My Drive','Colab Notebooks', 'Interim')\n","raw_path = os.path.join('drive','My Drive','Colab Notebooks','Raw')\n","if not os.path.exists(interim_path):\n","  os.makedirs(interim_path)\n","\n","files = os.listdir(interim_path)\n","print(files)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['collected.jsonl', 'cleaned.jsonl']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"joPU6NCVqKQR"},"source":["def remove(x: str) -> str:\n","    \"\"\"A function that cleans a give string by removing multiple punctuation, \n","    newlines, emojis, urls, and etceteras.\"\"\"\n","\n","    punct = list(\":;,.!$%^&#*()\\\"\\'[]{}\")\n","    punct += ['~',' | ']\n","    \n","    x = demoji(x)\n","    if '~' in x:\n","        x = x.replace('~','')\n","    x = x.replace('\\n',' ').replace('...','').split(' ')\n","    x = ' '.join([i for i in x if not i.startswith('http')])\n","    x = x.replace('  ',' ').lower()\n","    for p in punct:\n","        x = x.replace(p, '')\n","    x = x.strip()\n","    return x\n","\n","def demoji(text:str) -> str:\n","    regrex_pattern = re.compile(\n","    \"[\"\n","    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","    \"\\U0001F600-\\U0001F64F\"  # emoticons\n","    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","    \"\\U00002702-\\U000027B0\"  # Dingbats\n","    \"\\U000024C2-\\U0001F251\"\n","    \"\\u2010-\\u201f\"\n","    \"\\u2000-\\u200f\"\n","    \"]+\"\n","    )\n","    return regrex_pattern.sub(r'',text)\n","\n","def dehash(x: dict) -> str:\n","    \"\"\"A function that takes a \"\"\"\n","    y = ','.join([i['text'].lower() for i in x])\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQ_LwwGdLtCc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607042345499,"user_tz":300,"elapsed":13851,"user":{"displayName":"Collin Kovacs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDgAXzo7M5lUBaQyUGIfe8abPgCc4Ro0dDc6j6Ug=s64","userId":"13043862019868328475"}},"outputId":"ddaf8c1a-8b84-4cc6-91f7-4e0c23b8bc05"},"source":["# get the path for the collected data \n","collected_path = os.path.join(interim_path, 'collected.jsonl')\n","\n","# get the path for the new clean data to be written to\n","clean_path = os.path.join(interim_path, 'cleaned.jsonl')\n","\n","states_path = os.path.join(raw_path,'states.csv')\n","\n","state_list = []\n","with open(states_path, 'r') as states:\n","    csvReader = csv.reader(states)\n","    next(csvReader)\n","    for line in csvReader:\n","        state_list.append(line[:2])\n","\n","# instantiate cleaned.jsonl\n","with open(clean_path, 'w') as clean:\n","\n","    # open the collected.jsonl\n","    with open(collected_path, 'r') as file:\n","        \n","        lines = file.readlines()\n","        # iterate over each line\n","        for l in tqdm(range(len(lines))):\n","            \n","            line = lines[l]\n","            \n","            # parse each line to be a dictionary with .loads()\n","            j_line = json.loads(line)\n","\n","            # remove any line that does not have a location\n","            if j_line['location'] != \"\":\n","\n","                for state in state_list:\n","                    if state[0] in j_line['location'] or state[1] in j_line['location'] and 'D.C.' not in j_line['location'] and 'DC' not in j_line['location']:\n","                        j_line['text'] = remove(j_line['text'])\n","\n","                        # extract and lower the hashtags from list of dictionaries of hashtags\n","                        j_line['hashtags'] = dehash(j_line['hashtags'])\n","                        \n","                        j_line['standardized_loc'] = state[0]\n","\n","                        # write line to the clean.jsonl file\n","                        json.dump(j_line, clean)\n","                        clean.write('\\n')\n","                        \n","                        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 233618/233618 [00:12<00:00, 18192.78it/s]\n"],"name":"stderr"}]}]}